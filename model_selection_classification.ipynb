{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-29T22:18:49.123517Z",
     "start_time": "2025-12-29T22:18:49.110384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import optuna\n",
    "import warnings\n",
    "from dython.nominal import associations\n",
    "from sklearn.utils import all_estimators\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, accuracy_score, classification_report, precision_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "optuna.logging.set_verbosity(logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T22:18:49.433683Z",
     "start_time": "2025-12-29T22:18:49.305594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('final_datasets/enhanced_anxiety_dataset.csv')\n",
    "# df = pd.read_csv('cleaned_enhanced_anxiety_dataset.csv')\n",
    "target_column = 'Anxiety Level (1-10)'\n",
    "df[target_column] = df[target_column].astype('object')\n",
    "\n",
    "print(\"Total rows:\", df.shape[0])\n",
    "print(\"Total columns:\", df.shape[1])\n",
    "print(\"Target column:\", target_column)"
   ],
   "id": "bdd0f53410c1cde1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 11000\n",
      "Total columns: 19\n",
      "Target column: Anxiety Level (1-10)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T22:18:50.128393Z",
     "start_time": "2025-12-29T22:18:50.094598Z"
    }
   },
   "cell_type": "code",
   "source": "df.head()",
   "id": "83f6c30814e0e62a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Age  Gender Occupation  Sleep Hours  Physical Activity (hrs/week)  \\\n",
       "0   29  Female     Artist          6.0                           2.7   \n",
       "1   46   Other      Nurse          6.2                           5.7   \n",
       "2   64    Male      Other          5.0                           3.7   \n",
       "3   20  Female  Scientist          5.8                           2.8   \n",
       "4   49  Female      Other          8.2                           2.3   \n",
       "\n",
       "   Caffeine Intake (mg/day)  Alcohol Consumption (drinks/week) Smoking  \\\n",
       "0                       181                                 10     Yes   \n",
       "1                       200                                  8     Yes   \n",
       "2                       117                                  4      No   \n",
       "3                       360                                  6     Yes   \n",
       "4                       247                                  4     Yes   \n",
       "\n",
       "  Family History of Anxiety  Stress Level (1-10)  Heart Rate (bpm)  \\\n",
       "0                        No                   10               114   \n",
       "1                       Yes                    1                62   \n",
       "2                       Yes                    1                91   \n",
       "3                        No                    4                86   \n",
       "4                        No                    1                98   \n",
       "\n",
       "   Breathing Rate (breaths/min)  Sweating Level (1-5) Dizziness Medication  \\\n",
       "0                            14                     4        No        Yes   \n",
       "1                            23                     2       Yes         No   \n",
       "2                            28                     3        No         No   \n",
       "3                            17                     3        No         No   \n",
       "4                            19                     4       Yes        Yes   \n",
       "\n",
       "   Therapy Sessions (per month) Recent Major Life Event  Diet Quality (1-10)  \\\n",
       "0                             3                     Yes                    7   \n",
       "1                             2                      No                    8   \n",
       "2                             1                     Yes                    1   \n",
       "3                             0                      No                    1   \n",
       "4                             1                      No                    3   \n",
       "\n",
       "  Anxiety Level (1-10)  \n",
       "0                  5.0  \n",
       "1                  3.0  \n",
       "2                  1.0  \n",
       "3                  2.0  \n",
       "4                  1.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Sleep Hours</th>\n",
       "      <th>Physical Activity (hrs/week)</th>\n",
       "      <th>Caffeine Intake (mg/day)</th>\n",
       "      <th>Alcohol Consumption (drinks/week)</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>Family History of Anxiety</th>\n",
       "      <th>Stress Level (1-10)</th>\n",
       "      <th>Heart Rate (bpm)</th>\n",
       "      <th>Breathing Rate (breaths/min)</th>\n",
       "      <th>Sweating Level (1-5)</th>\n",
       "      <th>Dizziness</th>\n",
       "      <th>Medication</th>\n",
       "      <th>Therapy Sessions (per month)</th>\n",
       "      <th>Recent Major Life Event</th>\n",
       "      <th>Diet Quality (1-10)</th>\n",
       "      <th>Anxiety Level (1-10)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "      <td>Female</td>\n",
       "      <td>Artist</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>181</td>\n",
       "      <td>10</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>10</td>\n",
       "      <td>114</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>Other</td>\n",
       "      <td>Nurse</td>\n",
       "      <td>6.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "      <td>8</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>Male</td>\n",
       "      <td>Other</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>Female</td>\n",
       "      <td>Scientist</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>360</td>\n",
       "      <td>6</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>4</td>\n",
       "      <td>86</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49</td>\n",
       "      <td>Female</td>\n",
       "      <td>Other</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>247</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T22:18:50.462969Z",
     "start_time": "2025-12-29T22:18:50.418086Z"
    }
   },
   "cell_type": "code",
   "source": "df.info()",
   "id": "98201f16075fd953",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11000 entries, 0 to 10999\n",
      "Data columns (total 19 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   Age                                11000 non-null  int64  \n",
      " 1   Gender                             11000 non-null  object \n",
      " 2   Occupation                         11000 non-null  object \n",
      " 3   Sleep Hours                        11000 non-null  float64\n",
      " 4   Physical Activity (hrs/week)       11000 non-null  float64\n",
      " 5   Caffeine Intake (mg/day)           11000 non-null  int64  \n",
      " 6   Alcohol Consumption (drinks/week)  11000 non-null  int64  \n",
      " 7   Smoking                            11000 non-null  object \n",
      " 8   Family History of Anxiety          11000 non-null  object \n",
      " 9   Stress Level (1-10)                11000 non-null  int64  \n",
      " 10  Heart Rate (bpm)                   11000 non-null  int64  \n",
      " 11  Breathing Rate (breaths/min)       11000 non-null  int64  \n",
      " 12  Sweating Level (1-5)               11000 non-null  int64  \n",
      " 13  Dizziness                          11000 non-null  object \n",
      " 14  Medication                         11000 non-null  object \n",
      " 15  Therapy Sessions (per month)       11000 non-null  int64  \n",
      " 16  Recent Major Life Event            11000 non-null  object \n",
      " 17  Diet Quality (1-10)                11000 non-null  int64  \n",
      " 18  Anxiety Level (1-10)               11000 non-null  object \n",
      "dtypes: float64(2), int64(9), object(8)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T22:18:51.637774Z",
     "start_time": "2025-12-29T22:18:51.619302Z"
    }
   },
   "cell_type": "code",
   "source": "df.isnull().sum()",
   "id": "15a22426fac9cd51",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                                  0\n",
       "Gender                               0\n",
       "Occupation                           0\n",
       "Sleep Hours                          0\n",
       "Physical Activity (hrs/week)         0\n",
       "Caffeine Intake (mg/day)             0\n",
       "Alcohol Consumption (drinks/week)    0\n",
       "Smoking                              0\n",
       "Family History of Anxiety            0\n",
       "Stress Level (1-10)                  0\n",
       "Heart Rate (bpm)                     0\n",
       "Breathing Rate (breaths/min)         0\n",
       "Sweating Level (1-5)                 0\n",
       "Dizziness                            0\n",
       "Medication                           0\n",
       "Therapy Sessions (per month)         0\n",
       "Recent Major Life Event              0\n",
       "Diet Quality (1-10)                  0\n",
       "Anxiety Level (1-10)                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T22:18:52.831568Z",
     "start_time": "2025-12-29T22:18:52.799818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def unic_columns(data):\n",
    "    object_columns = data.select_dtypes(include='object').columns\n",
    "    for column in object_columns:\n",
    "        print(f\"Унікальні значення в колонці: '{column}' : \")\n",
    "        print(data[column].value_counts())\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "unic_columns(df)"
   ],
   "id": "b28aa9b45a73a2f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Унікальні значення в колонці: 'Gender' : \n",
      "Gender\n",
      "Female    3730\n",
      "Male      3657\n",
      "Other     3613\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Унікальні значення в колонці: 'Occupation' : \n",
      "Occupation\n",
      "Musician      892\n",
      "Artist        888\n",
      "Student       878\n",
      "Nurse         861\n",
      "Chef          858\n",
      "Doctor        842\n",
      "Other         840\n",
      "Freelancer    838\n",
      "Engineer      833\n",
      "Scientist     832\n",
      "Athlete       822\n",
      "Lawyer        809\n",
      "Teacher       807\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Унікальні значення в колонці: 'Smoking' : \n",
      "Smoking\n",
      "Yes    5779\n",
      "No     5221\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Унікальні значення в колонці: 'Family History of Anxiety' : \n",
      "Family History of Anxiety\n",
      "Yes    5847\n",
      "No     5153\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Унікальні значення в колонці: 'Dizziness' : \n",
      "Dizziness\n",
      "Yes    5672\n",
      "No     5328\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Унікальні значення в колонці: 'Medication' : \n",
      "Medication\n",
      "Yes    5666\n",
      "No     5334\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Унікальні значення в колонці: 'Recent Major Life Event' : \n",
      "Recent Major Life Event\n",
      "Yes    5623\n",
      "No     5377\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Унікальні значення в колонці: 'Anxiety Level (1-10)' : \n",
      "Anxiety Level (1-10)\n",
      "4.0     2416\n",
      "3.0     2407\n",
      "2.0     1756\n",
      "5.0     1629\n",
      "1.0     1039\n",
      "6.0      616\n",
      "8.0      363\n",
      "9.0      329\n",
      "10.0     322\n",
      "7.0      123\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-12-29T22:18:53.567431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def corr_dif(data, threshold=0.85):\n",
    "    correlation_matrix = associations(data)\n",
    "    \n",
    "    def plot_lower_triangle_corr_matrix(corr_matrix):\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "        plt.figure(figsize=(12, 10))  \n",
    "        sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', square=True)\n",
    "        plt.title(\"Нижня трикутна кореляційна матриця\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('corr_matrix.png', dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "    plot_lower_triangle_corr_matrix(correlation_matrix['corr'])\n",
    "    \n",
    "    def get_highly_correlated_pairs(corr_matrix, threshold=0.85):\n",
    "        corr_pairs = []\n",
    "        cols = corr_matrix.columns\n",
    "        for i in range(len(cols)):\n",
    "            for j in range(i):\n",
    "                corr_value = corr_matrix.iloc[i, j]\n",
    "                if abs(corr_value) >= threshold:\n",
    "                    corr_pairs.append((cols[i], cols[j], corr_value))\n",
    "        return sorted(corr_pairs, key=lambda x: -abs(x[2]))\n",
    "    \n",
    "    highly_correlated_pairs = get_highly_correlated_pairs(correlation_matrix['corr'], threshold)\n",
    "    \n",
    "    print(\"\\nВисококорельовані пари ознак:\")\n",
    "    for col1, col2, value in highly_correlated_pairs:\n",
    "        print(f\"{col1} - {col2} --> кореляція {value:.2f}\")\n",
    "        \n",
    "    target_correlations = correlation_matrix['corr'].iloc[:, -1]\n",
    "    target_col = target_correlations.name\n",
    "    \n",
    "    corr_df = target_correlations.to_frame()\n",
    "    corr_df.columns = ['Correlations']\n",
    "    corr_df['Abs Correlation'] = corr_df['Correlations'].abs()\n",
    "    sorted_corr_df = corr_df.sort_values(by='Abs Correlation', ascending=False)\n",
    "    sorted_corr_df = sorted_corr_df.drop(columns=['Abs Correlation'])\n",
    "    print(f\"\\nКоефіцієнт кореляції відносно цільового поля {target_col}\")\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    print(sorted_corr_df)\n",
    "    \n",
    "    return correlation_matrix\n",
    "\n",
    "correlation_result = corr_dif(df, 0.8)"
   ],
   "id": "621b1eb61b474ee3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T22:12:19.888862Z",
     "start_time": "2025-12-29T22:12:19.874316Z"
    }
   },
   "cell_type": "code",
   "source": "df_2 = df.copy()",
   "id": "b5e9603c9173d023",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T22:12:25.128652Z",
     "start_time": "2025-12-29T22:12:25.088961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_original = 'Anxiety Level (1-10)'\n",
    "def categorize_anxiety(level):\n",
    "    level = int(level) if isinstance(level, str) else level\n",
    "    if level <= 4:\n",
    "        return 'Low'\n",
    "    elif 4 < level <= 7:\n",
    "        return 'Medium'\n",
    "    else: \n",
    "        return 'High'\n",
    "\n",
    "df['Anxiety Class'] = df[target_original].apply(categorize_anxiety).astype('category')\n",
    "target_col = 'Anxiety Class'\n",
    "df = df.drop(columns=[target_original])\n",
    "\n",
    "numeric_cols = df.select_dtypes(include='number').columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "if target_col in categorical_cols:\n",
    "    categorical_cols.remove(target_col)\n",
    "\n",
    "print(f\"Числові колонки ({len(numeric_cols)}): {numeric_cols}\")\n",
    "print(f\"Категоріальні колонки ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"Цільова змінна: {target_col}\")"
   ],
   "id": "96bf6570d24fd1d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Числові колонки (11): ['Age', 'Sleep Hours', 'Physical Activity (hrs/week)', 'Caffeine Intake (mg/day)', 'Alcohol Consumption (drinks/week)', 'Stress Level (1-10)', 'Heart Rate (bpm)', 'Breathing Rate (breaths/min)', 'Sweating Level (1-5)', 'Therapy Sessions (per month)', 'Diet Quality (1-10)']\n",
      "Категоріальні колонки (7): ['Gender', 'Occupation', 'Smoking', 'Family History of Anxiety', 'Dizziness', 'Medication', 'Recent Major Life Event']\n",
      "Цільова змінна: Anxiety Class\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "створюжм 3 датасети на кожен клас і в них визначаємо аутлаєри (95% варіансу має пояснення) і за допомогою Isolation forest видаляємо аутлаєри, далі цих три датасети конкатенуємо назад в один, також візуалізувати три кластери з цими класами",
   "id": "9750bb4a0c251f3a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T22:12:30.950738Z",
     "start_time": "2025-12-29T22:12:30.903899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label_encoders = {}\n",
    "df_encoded = df.copy()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col] = le.fit_transform(df[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"Закодовано {col}: {len(le.classes_)} унікальних значень\")\n",
    "\n",
    "le_y = LabelEncoder()\n",
    "y = le_y.fit_transform(df[target_col])\n",
    "print(f\"\\nКласи цільової змінної: {le_y.classes_}\")\n",
    "print(f\"Розподіл класів: {np.bincount(y)}\")"
   ],
   "id": "3ef9bfbb35e81881",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Закодовано Gender: 3 унікальних значень\n",
      "Закодовано Occupation: 13 унікальних значень\n",
      "Закодовано Smoking: 2 унікальних значень\n",
      "Закодовано Family History of Anxiety: 2 унікальних значень\n",
      "Закодовано Dizziness: 2 унікальних значень\n",
      "Закодовано Medication: 2 унікальних значень\n",
      "Закодовано Recent Major Life Event: 2 унікальних значень\n",
      "\n",
      "Класи цільової змінної: ['High' 'Low' 'Medium']\n",
      "Розподіл класів: [1014 7618 2368]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T22:12:38.522735Z",
     "start_time": "2025-12-29T22:12:38.486915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scaler = StandardScaler()\n",
    "df_encoded[numeric_cols] = scaler.fit_transform(df_encoded[numeric_cols])\n",
    "\n",
    "print(\"Масштабування числових ознак завершено\")\n",
    "print(f\"Середнє значення після масштабування:\\n{df_encoded[numeric_cols].mean()}\")\n",
    "print(f\"\\nСтандартне відхилення після масштабування:\\n{df_encoded[numeric_cols].std()}\")"
   ],
   "id": "d97c39612b0862cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Масштабування числових ознак завершено\n",
      "Середнє значення після масштабування:\n",
      "Age                                 -1.401707e-16\n",
      "Sleep Hours                         -9.818409e-17\n",
      "Physical Activity (hrs/week)        -7.492996e-17\n",
      "Caffeine Intake (mg/day)             1.724681e-16\n",
      "Alcohol Consumption (drinks/week)   -1.111030e-16\n",
      "Stress Level (1-10)                 -3.035955e-17\n",
      "Heart Rate (bpm)                     1.821573e-16\n",
      "Breathing Rate (breaths/min)         2.306034e-16\n",
      "Sweating Level (1-5)                -1.498599e-16\n",
      "Therapy Sessions (per month)         5.748937e-17\n",
      "Diet Quality (1-10)                  3.229740e-17\n",
      "dtype: float64\n",
      "\n",
      "Стандартне відхилення після масштабування:\n",
      "Age                                  1.000045\n",
      "Sleep Hours                          1.000045\n",
      "Physical Activity (hrs/week)         1.000045\n",
      "Caffeine Intake (mg/day)             1.000045\n",
      "Alcohol Consumption (drinks/week)    1.000045\n",
      "Stress Level (1-10)                  1.000045\n",
      "Heart Rate (bpm)                     1.000045\n",
      "Breathing Rate (breaths/min)         1.000045\n",
      "Sweating Level (1-5)                 1.000045\n",
      "Therapy Sessions (per month)         1.000045\n",
      "Diet Quality (1-10)                  1.000045\n",
      "dtype: float64\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T20:37:33.562680Z",
     "start_time": "2025-12-29T20:37:33.553604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # from sklearn.ensemble import IsolationForest\n",
    "# # \n",
    "# # df_cleaned_list = []\n",
    "# # contamination_rate = 0.05\n",
    "# # \n",
    "# # for class_label in np.unique(y):\n",
    "# #     class_mask = (y == class_label)\n",
    "# #     X_class = df_encoded.drop(columns=[target_col])[class_mask]\n",
    "# #     y_class = y[class_mask]\n",
    "# #     \n",
    "# #     iso = IsolationForest(contamination=contamination_rate, random_state=42)\n",
    "# #     outlier_preds = iso.fit_predict(X_class)\n",
    "# #     \n",
    "# #     X_class_cleaned = X_class[outlier_preds == 1]\n",
    "# #     y_class_cleaned = y_class[outlier_preds == 1]\n",
    "# #     \n",
    "# #     temp_df = X_class_cleaned.copy()\n",
    "# #     temp_df[target_col] = y_class_cleaned\n",
    "# #     df_cleaned_list.append(temp_df)\n",
    "# #     \n",
    "# #     print(f\"Клас '{le_y.classes_[class_label]}': видалено {sum(outlier_preds == -1)} аутлаєрів\")\n",
    "# # \n",
    "# # df_final_cleaned = pd.concat(df_cleaned_list, axis=0).sample(frac=1, random_state=42)\n",
    "# # df_final_cleaned.to_csv('final_cleaned.csv', index=False)\n",
    "# # X = df_final_cleaned.drop(columns=[target_col])\n",
    "# # y = df_final_cleaned[target_col].values\n",
    "# # from imblearn.under_sampling import EditedNearestNeighbors\n",
    "# # X_full = df_encoded.drop(columns=[target_col])\n",
    "# # y_full = y # використовуємо вже закодований y\n",
    "# \n",
    "# # print(f\"Початкова кількість записів: {len(X_full)}\")\n",
    "# \n",
    "# # 2. Використовуємо алгоритм сімейства Nearest Neighbors для видалення аутлаєрів\n",
    "# # EditedNearestNeighbors видаляє зразки, чий клас відрізняється від більшості сусідів.\n",
    "# # Це ідеально підходить для очищення \"шуму\" та аутлаєрів на межах.\n",
    "# # enn = EditedNearestNeighbors(n_neighbors=3)\n",
    "# # X_resampled, y_resampled = enn.fit_resample(X_full, y_full)\n",
    "# \n",
    "# # Оскільки ENN видаляє стільки, скільки вважає за потрібне, перевіримо результат\n",
    "# # print(f\"Кількість записів після ENN: {len(X_resampled)}\")\n",
    "# # print(f\"Видалено всього: {len(X_full) - len(X_resampled)} рядків\")\n",
    "# \n",
    "# # 3. Якщо вам потрібно видалити СТРОГО 5% (як ви просили раніше)\n",
    "# # Nearest Neighbors не гарантує фіксований відсоток, але він набагато точніше \n",
    "# # знаходить аномалії в ненормальному розподілі, ніж статистичні методи.\n",
    "# \n",
    "# # 4. Формуємо фінальний очищений датасет\n",
    "# # df_final_cleaned = X_resampled.copy()\n",
    "# # df_final_cleaned[target_col] = y_resampled\n",
    "# # \n",
    "# # # Перемішуємо дані\n",
    "# # df_final_cleaned = df_final_cleaned.sample(frac=1, random_state=42)\n",
    "# # \n",
    "# # # Оновлюємо змінні для навчання\n",
    "# # X = df_final_cleaned.drop(columns=[target_col])\n",
    "# # y = df_final_cleaned[target_col].values\n",
    "# # X = df_encoded.drop(columns=[target_col])\n",
    "# \n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "# \n",
    "# # 1. Підготовка даних\n",
    "# X_full = df_encoded.drop(columns=[target_col])\n",
    "# y_full = y\n",
    "# \n",
    "# # 2. Реалізація логіки Edited Nearest Neighbors (ENN) вручну\n",
    "# # Ми видаляємо точки, чий клас не збігається з більшістю з 3-х найближчих сусідів\n",
    "# n_neighbors = 3\n",
    "# nn = NearestNeighbors(n_neighbors=n_neighbors + 1) # +1, бо точка є сусідом сама собі\n",
    "# nn.fit(X_full)\n",
    "# \n",
    "# # Знаходимо індекси сусідів для кожної точки\n",
    "# distances, indices = nn.kneighbors(X_full)\n",
    "# \n",
    "# # Маска для тих, кого ми залишимо\n",
    "# keep_mask = []\n",
    "# \n",
    "# for i in range(len(X_full)):\n",
    "#     # Індекси сусідів (пропускаємо перший індекс, бо це сама точка)\n",
    "#     neighbor_indices = indices[i][1:]\n",
    "#     # Класи сусідів\n",
    "#     neighbor_classes = y_full[neighbor_indices]\n",
    "#     \n",
    "#     # Якщо клас точки збігається з більшістю сусідів - залишаємо\n",
    "#     # (Наприклад, якщо хоча б 2 з 3 сусідів мають такий самий клас)\n",
    "#     most_common_neighbor_class = np.bincount(neighbor_classes).argmax()\n",
    "#     \n",
    "#     if y_full[i] == most_common_neighbor_class:\n",
    "#         keep_mask.append(True)\n",
    "#     else:\n",
    "#         keep_mask.append(False)\n",
    "# \n",
    "# keep_mask = np.array(keep_mask)\n",
    "# \n",
    "# # 3. Формуємо очищений датасет\n",
    "# X_resampled = X_full[keep_mask]\n",
    "# y_resampled = y_full[keep_mask]\n",
    "# \n",
    "# print(f\"Початкова кількість записів: {len(X_full)}\")\n",
    "# print(f\"Видалено 'шумних' точок (NN-outliers): {len(X_full) - len(X_resampled)}\")\n",
    "# \n",
    "# df_final_cleaned = X_resampled.copy()\n",
    "# df_final_cleaned[target_col] = y_resampled\n",
    "# df_final_cleaned[target_col] = le_y.inverse_transform(df_final_cleaned[target_col])\n",
    "# df_final_cleaned = df_final_cleaned.sample(frac=1, random_state=42)\n",
    "# df_final_cleaned.to_csv('anxiety_dataset_enn.csv')\n",
    "# # Оновлюємо змінні для подальшого коду\n",
    "# X = df_final_cleaned.drop(columns=[target_col])\n",
    "# y = df_final_cleaned[target_col].values\n",
    "# n_classes = len(np.unique(y))\n",
    "# is_multiclass = n_classes > 2\n",
    "# \n",
    "# print(f\"Кількість класів: {n_classes}\")\n",
    "# print(f\"Мультикласова класифікація: {is_multiclass}\")\n",
    "# \n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42, stratify=y\n",
    "# )\n",
    "# \n",
    "# print(f\"\\nРозмір тренувальної вибірки: {X_train.shape}\")\n",
    "# print(f\"Розмір тестової вибірки: {X_test.shape}\")\n",
    "# # print(f\"Розподіл класів у тренувальній вибірці: {np.bincount(y_train)}\")\n",
    "# # print(f\"Розподіл класів у тестовій вибірці: {np.bincount(y_test)}\")"
   ],
   "id": "f266942e6d1756a4",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T22:12:44.293067Z",
     "start_time": "2025-12-29T22:12:44.257095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = df_encoded.drop(columns=[target_col])\n",
    "n_classes = len(np.unique(y))\n",
    "is_multiclass = n_classes > 2\n",
    "\n",
    "print(f\"Кількість класів: {n_classes}\")\n",
    "print(f\"Мультикласова класифікація: {is_multiclass}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nРозмір тренувальної вибірки: {X_train.shape}\")\n",
    "print(f\"Розмір тестової вибірки: {X_test.shape}\")\n",
    "print(f\"Розподіл класів у тренувальній вибірці: {np.bincount(y_train)}\")\n",
    "print(f\"Розподіл класів у тестовій вибірці: {np.bincount(y_test)}\")"
   ],
   "id": "9da475ad355cd7a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кількість класів: 3\n",
      "Мультикласова класифікація: True\n",
      "\n",
      "Розмір тренувальної вибірки: (8800, 18)\n",
      "Розмір тестової вибірки: (2200, 18)\n",
      "Розподіл класів у тренувальній вибірці: [ 811 6095 1894]\n",
      "Розподіл класів у тестовій вибірці: [ 203 1523  474]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-12-29T22:12:54.658251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "classifiers = all_estimators(type_filter='classifier')\n",
    "skip_models = ['GaussianProcessClassifier']\n",
    "results = {}\n",
    "\n",
    "print(f\"Тестування різних класифікаторів з кросвалідацією (5 фолдів)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "CV = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, ClassifierClass in classifiers:\n",
    "    if name in skip_models:\n",
    "        continue \n",
    "    try:\n",
    "        model = ClassifierClass()\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=CV, scoring='precision_macro', n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        roc_auc = np.nan\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            pred_proba = model.predict_proba(X_train)\n",
    "            if is_multiclass:\n",
    "                if pred_proba.ndim == 2 and pred_proba.shape[1] == n_classes:\n",
    "                    roc_auc = roc_auc_score(y_train, pred_proba, multi_class='ovr')\n",
    "                else:\n",
    "                    roc_auc = np.nan\n",
    "            else:\n",
    "                if pred_proba.ndim == 2 and pred_proba.shape[1] == 2:\n",
    "                    roc_auc = roc_auc_score(y_train, pred_proba[:, 1])\n",
    "                elif pred_proba.ndim == 1:\n",
    "                    roc_auc = roc_auc_score(y_train, pred_proba)\n",
    "                else:\n",
    "                    roc_auc = np.nan\n",
    "        elif hasattr(model, 'decision_function'): \n",
    "            pred_score = model.decision_function(X_train)\n",
    "            if is_multiclass:\n",
    "                if pred_score.ndim == 2 and pred_score.shape[1] == n_classes:\n",
    "                    roc_auc = roc_auc_score(y_train, pred_score, multi_class='ovr')\n",
    "                else:\n",
    "                    roc_auc = np.nan\n",
    "            else:\n",
    "                if pred_score.ndim == 1:\n",
    "                    roc_auc = roc_auc_score(y_train, pred_score)\n",
    "                elif pred_score.ndim == 2 and pred_score.shape[1] == 2:\n",
    "                    roc_auc = roc_auc_score(y_train, pred_score[:, 1])\n",
    "                else:\n",
    "                    roc_auc = np.nan\n",
    "        else:\n",
    "            roc_auc = np.nan\n",
    "            \n",
    "        if not np.isnan(scores.mean()):\n",
    "            results[name] = {\n",
    "                'mean_precision': scores.mean(),\n",
    "                'std_dev': scores.std(),\n",
    "                'roc_auc': roc_auc,\n",
    "            }\n",
    "            print(f\"{name}: MEAN PRECISION {scores.mean():.3f}, STD (± {scores.std():.3f}), ROC AUC: {roc_auc:.3f}\")\n",
    "        else:\n",
    "            print(f\"{name}: Пропущено через NaN у результаті\") \n",
    "\n",
    "    except Exception as e:\n",
    "        continue\n",
    "       \n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1]['mean_precision'], reverse=True)\n",
    "\n",
    "print(\"\\nТоп-10 найбільш точних моделей:\")\n",
    "for i, (name, metric) in enumerate(sorted_results[:10], 1):\n",
    "    print(f\"{i:2d}. {name:40s} | ACC: {metric['mean_precision']:.3f} (± {metric['std_dev']:.3f}) | ROC-AUC: {metric['roc_auc']:.3f}\")\n",
    "    \n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')"
   ],
   "id": "17b3ac632ae69817",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тестування різних класифікаторів з кросвалідацією (5 фолдів)\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "top5 = sorted_results[:5]\n",
    "print(\"\\nТОП-5 моделей для ансамблів:\")\n",
    "for name, metric in top5:\n",
    "    print(name, \"| precision =\", metric['mean_precision'])\n",
    "\n",
    "def instantiate_model(model_name):\n",
    "    for name, Classifier in classifiers:\n",
    "        if name == model_name:\n",
    "            try:\n",
    "                return Classifier()\n",
    "            except:\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "top_models = []\n",
    "for name, metric in top5:\n",
    "    model_obj = instantiate_model(name)\n",
    "    if model_obj is not None:\n",
    "        top_models.append((name, model_obj))\n",
    "\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=top_models,\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "voting_model.fit(X_train, y_train)\n",
    "y_pred_vote = voting_model.predict(X_test)\n",
    "\n",
    "precision_vote = precision_score(y_test, y_pred_vote, average=\"macro\", zero_division=0)\n",
    "print(f\"\\nPrecision (Voting, soft) = {precision_vote:.3f}\")\n",
    "\n",
    "stacked_model = StackingClassifier(\n",
    "    estimators=top_models,\n",
    "    final_estimator=LogisticRegression(max_iter=1000, random_state=42)\n",
    ")\n",
    "\n",
    "stacked_model.fit(X_train, y_train)\n",
    "y_pred_stack = stacked_model.predict(X_test)\n",
    "\n",
    "precision_stack = precision_score(y_test, y_pred_stack, average=\"macro\", zero_division=0)\n",
    "print(f\"Precision (Stacking) = {precision_stack:.3f}\")"
   ],
   "id": "639d12842ba4d47f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "param_grids = {\n",
    "    \"QuadraticDiscriminantAnalysis\": { #\n",
    "        'reg_param': [0.0, 0.1, 0.2, 0.3, 0.5, 0.7],\n",
    "        'store_covariance': [True, False],\n",
    "        'tol': [1e-4, 1e-3, 1e-2]\n",
    "    },\n",
    "    \"LinearDiscriminantAnalysis\": { #\n",
    "        'solver': ['svd', 'lsqr', 'eigen'],\n",
    "        'shrinkage': [None, 'auto', 0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "        'tol': [1e-4, 1e-3],\n",
    "    },\n",
    "    \"LogisticRegressionCV\": {\n",
    "        'Cs': [10, 20],\n",
    "        'max_iter': [1000, 2000, 3000],\n",
    "        'solver': ['lbfgs', 'liblinear', 'saga'],\n",
    "        'penalty': ['l2', 'l1'],\n",
    "        'cv': [3, 5]\n",
    "    },\n",
    "    \"GaussianNB\": { #\n",
    "        'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]\n",
    "    },\n",
    "    \"MLPClassifier\": { #\n",
    "        'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'solver': ['adam'],\n",
    "        'alpha': [0.0001, 0.001, 0.01],\n",
    "        'max_iter': [500, 1000]\n",
    "    },\n",
    "    \"RandomForestClassifier\": { #\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2', None]\n",
    "    },\n",
    "    \"GradientBoostingClassifier\": { #\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "    },\n",
    "    \"LogisticRegression\": { #\n",
    "        'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "        'max_iter': [1000, 2000],\n",
    "        'solver': ['lbfgs', 'liblinear', 'saga'],\n",
    "        'penalty': ['l2', 'l1'],\n",
    "        'class_weight': [None, 'balanced']\n",
    "    },\n",
    "    \"ExtraTreesClassifier\": { #\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "    },\n",
    "    \"CalibratedClassifierCV\": {\n",
    "        'method': ['sigmoid', 'isotonic'],\n",
    "        'cv': [3, 5]\n",
    "    },\n",
    "    \"HistGradientBoostingClassifier\": { #\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [5, 10, 15, None],\n",
    "        'max_iter': [100, 200, 300],\n",
    "        \"l2_regularization\": [0.0, 1.0, 5.0]\n",
    "    },\n",
    "    \"AdaBoostClassifier\": { #\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1, 0.5, 1.0, 1.5],\n",
    "        'algorithm': ['SAMME', 'SAMME.R']\n",
    "    },\n",
    "    \"KNeighborsClassifier\": { #\n",
    "        'n_neighbors': [3, 5, 7, 9, 11, 15],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree'],\n",
    "        'leaf_size': [20, 30, 40, 50],\n",
    "        'p': [1, 2]\n",
    "    },\n",
    "    \"DecisionTreeClassifier\": { #\n",
    "        'max_depth': [5, 10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5, 10, 20],\n",
    "        'min_samples_leaf': [1, 2, 4, 8],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_features': ['sqrt', 'log2', None]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Гіперпараметри підготовлені для наступних моделей:\")\n",
    "for model_name in param_grids.keys():\n",
    "    print(f\"  - {model_name}: {len(param_grids[model_name])} параметрів\")"
   ],
   "id": "f1446767b964a56d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "top_10_classifiers = sorted_results[:10]\n",
    "CV = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "feature_names = X_train.columns\n",
    "feature_importance_dict = {}\n",
    "tuned_results = {}\n",
    "\n",
    "print(\"Початок підбору гіперпараметрів для Top-10 моделей\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for name, _ in top_10_classifiers:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Обробка моделі: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    ClassifierClass = dict(classifiers)[name]\n",
    "    base_model = ClassifierClass()\n",
    "    \n",
    "    if name in param_grids:\n",
    "        param_grid = param_grids[name]\n",
    "        print(f\"Підбір гіперпараметрів: {len(param_grid)} параметрів\")\n",
    "        print(f\"Параметри: {list(param_grid.keys())}\")\n",
    "        \n",
    "        try:\n",
    "            grid_search = GridSearchCV(\n",
    "                base_model, \n",
    "                param_grid, \n",
    "                cv=CV, \n",
    "                scoring='precision_macro',\n",
    "                n_jobs=-1,\n",
    "                verbose=1\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            best_params = grid_search.best_params_\n",
    "            best_idx = grid_search.best_index_\n",
    "            mean_score = grid_search.cv_results_['mean_test_score'][best_idx]\n",
    "            std_score = grid_search.cv_results_['std_test_score'][best_idx]\n",
    "        except Exception as e:\n",
    "            print(f\"Помилка GridSearch: {e}\")\n",
    "            print(\"Використовується модель за замовчуванням\")\n",
    "            best_model = base_model\n",
    "            best_model.fit(X_train, y_train)\n",
    "            best_params = \"Параметри за замовчуванням\"\n",
    "            scores = cross_val_score(best_model, X_train, y_train, cv=CV, scoring='precision_macro', n_jobs=-1)\n",
    "            mean_score = scores.mean()\n",
    "            std_score = scores.std()\n",
    "    else:\n",
    "        print(f\"Гіперпараметри не визначені, використовується модель за замовчуванням\")\n",
    "        best_model = base_model\n",
    "        best_model.fit(X_train, y_train)\n",
    "        best_params = \"Параметри за замовчуванням\"\n",
    "        scores = cross_val_score(best_model, X_train, y_train, cv=CV, scoring='precision_macro', n_jobs=-1)\n",
    "        mean_score = scores.mean()\n",
    "        std_score = scores.std()\n",
    "    \n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    if hasattr(best_model, 'predict_proba'):\n",
    "        proba_test = best_model.predict_proba(X_test)\n",
    "        if is_multiclass:\n",
    "            if proba_test.ndim == 2 and proba_test.shape[1] == n_classes:\n",
    "                roc_auc_test = roc_auc_score(y_test, proba_test, multi_class='ovr')\n",
    "            else:\n",
    "                roc_auc_test = np.nan\n",
    "        else:\n",
    "            if proba_test.ndim == 2 and proba_test.shape[1] == 2:\n",
    "                roc_auc_test = roc_auc_score(y_test, proba_test[:, 1])\n",
    "            elif proba_test.ndim == 1:\n",
    "                roc_auc_test = roc_auc_score(y_test, proba_test)\n",
    "            else:\n",
    "                roc_auc_test = np.nan\n",
    "    elif hasattr(best_model, 'decision_function'):\n",
    "        proba_test = best_model.decision_function(X_test)\n",
    "        if is_multiclass:\n",
    "            if proba_test.ndim == 2 and proba_test.shape[1] == n_classes:\n",
    "                roc_auc_test = roc_auc_score(y_test, proba_test, multi_class='ovr')\n",
    "            else:\n",
    "                roc_auc_test = np.nan\n",
    "        else:\n",
    "            if proba_test.ndim == 1:\n",
    "                roc_auc_test = roc_auc_score(y_test, proba_test)\n",
    "            elif proba_test.ndim == 2 and proba_test.shape[1] == 2:\n",
    "                roc_auc_test = roc_auc_score(y_test, proba_test[:, 1])\n",
    "            else:\n",
    "                roc_auc_test = np.nan\n",
    "    else:\n",
    "        roc_auc_test = np.nan\n",
    "    \n",
    "    tuned_results[name] = {\n",
    "        'best_params': best_params,\n",
    "        'cv_mean_score': mean_score,\n",
    "        'cv_std_score': std_score,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'roc_auc_test': roc_auc_test,\n",
    "        'model': best_model\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(f\"Результати для {name}:\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    print(f\"Найкращі параметри: {best_params}\")\n",
    "    print(f\"CV precision_macro: {mean_score:.4f} (± {std_score:.4f})\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test ROC-AUC: {roc_auc_test:.4f}\")\n",
    "    \n",
    "    y_pred_full = best_model.predict(X)\n",
    "    df_2[f'{name}_prediction'] = le_y.inverse_transform(y_pred_full)\n",
    "    \n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(f\"Важливість ознак для {name}:\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    \n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        importances = best_model.feature_importances_\n",
    "        feature_importance_dict[name] = importances\n",
    "        sort_idx = np.argsort(importances)[::-1]\n",
    "        print(\"Топ-10 найважливіших ознак:\")\n",
    "        for i, idx in enumerate(sort_idx[:10], 1):\n",
    "            print(f\"{i:2d}. {feature_names[idx]:30s}: {importances[idx]:.4f}\")\n",
    "    else:\n",
    "        print(\"Обчислення Permutation Importance...\")\n",
    "        perm_importances = permutation_importance(\n",
    "            best_model, X_test, y_test, \n",
    "            n_repeats=10, \n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        importances = perm_importances.importances_mean\n",
    "        feature_importance_dict[name] = importances\n",
    "        sort_idx = np.argsort(importances)[::-1]\n",
    "        print(\"Топ-10 найважливіших ознак:\")\n",
    "        for i, idx in enumerate(sort_idx[:10], 1):\n",
    "            print(f\"{i:2d}. {feature_names[idx]:30s}: {importances[idx]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Підбір гіперпараметрів завершено!\")\n",
    "print(\"=\"*80)"
   ],
   "id": "59ef76460baa4cd7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def suggest_params(trial, model_name):\n",
    "    if model_name == \"QuadraticDiscriminantAnalysis\":\n",
    "        reg_param = trial.suggest_loguniform('reg_param', 1e-4, 0.7) \n",
    "        store_covariance = trial.suggest_categorical('store_covariance', [True, False])\n",
    "        tol = trial.suggest_categorical('tol', [1e-4, 1e-3, 1e-2])\n",
    "        \n",
    "        return {\n",
    "            'reg_param': reg_param,\n",
    "            'store_covariance': store_covariance,\n",
    "            'tol': tol\n",
    "        }\n",
    "    elif model_name == \"LinearDiscriminantAnalysis\":\n",
    "        solver = trial.suggest_categorical('solver', ['svd', 'lsqr', 'eigen'])\n",
    "        params = {'solver': solver}\n",
    "        if solver in ['lsqr', 'eigen']:\n",
    "            params['shrinkage'] = trial.suggest_categorical('shrinkage', ['auto',  0.1, 0.3, 0.5, 0.7, 0.9])\n",
    "            params['tol'] = trial.suggest_categorical('tol', [1e-4, 1e-3])  \n",
    "        return params\n",
    "        \n",
    "    elif model_name == \"LogisticRegression\":\n",
    "        solver = trial.suggest_categorical('solver', ['lbfgs', 'liblinear', 'saga'])\n",
    "        \n",
    "        if solver == 'lbfgs':\n",
    "            penalty = 'l2'\n",
    "        elif solver in ['liblinear', 'saga']:\n",
    "            penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])\n",
    "        else:\n",
    "            penalty = 'l2' \n",
    "            \n",
    "        C = trial.suggest_float('C', 0.001, 100.0, log=True)\n",
    "        max_iter = trial.suggest_int('max_iter', 1000, 3000)\n",
    "        class_weight = trial.suggest_categorical('class_weight', ['balanced', None])\n",
    "        return {'solver': solver, 'penalty': penalty, 'C': C, 'max_iter': max_iter, 'class_weight': class_weight}\n",
    "        \n",
    "    elif model_name == \"GaussianNB\":\n",
    "        var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-9, 1e-5)\n",
    "        return {'var_smoothing': var_smoothing}\n",
    "        \n",
    "    elif model_name == \"MLPClassifier\":\n",
    "        hidden_layer_sizes_choice = trial.suggest_categorical('hidden_layer_sizes_choice', ['small', 'medium', 'large'])\n",
    "        if hidden_layer_sizes_choice == 'small':\n",
    "            hidden_layer_sizes = (50,)\n",
    "        elif hidden_layer_sizes_choice == 'medium':\n",
    "            hidden_layer_sizes = (100,)\n",
    "        else:\n",
    "            hidden_layer_sizes = (50, 50) \n",
    "            \n",
    "        activation = trial.suggest_categorical('activation', ['relu', 'tanh'])\n",
    "        alpha = trial.suggest_loguniform('alpha', 0.0001, 0.01)\n",
    "        max_iter = trial.suggest_int('max_iter', 500, 1000)\n",
    "        solver = trial.suggest_categorical('solver', ['adam'])\n",
    "        \n",
    "        return {\n",
    "            'hidden_layer_sizes': hidden_layer_sizes,\n",
    "            'activation': activation,\n",
    "            'alpha': alpha,\n",
    "            'max_iter': max_iter,\n",
    "            'solver': solver\n",
    "        }\n",
    "        \n",
    "    elif model_name in [\"RandomForestClassifier\", \"ExtraTreesClassifier\"]:\n",
    "        n_estimators = trial.suggest_int('n_estimators', 100, 300)\n",
    "        max_depth = trial.suggest_categorical('max_depth', [10, 20, None])\n",
    "        min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "        min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 4)\n",
    "        max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
    "        \n",
    "        return {\n",
    "            'n_estimators': n_estimators,\n",
    "            'max_depth': max_depth,\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'min_samples_leaf': min_samples_leaf,\n",
    "            'max_features': max_features\n",
    "        }\n",
    "        \n",
    "    elif model_name == \"GradientBoostingClassifier\":\n",
    "        n_estimators = trial.suggest_int('n_estimators', 100, 300)\n",
    "        learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 0.2)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 7)\n",
    "        min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "        min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 4)\n",
    "        \n",
    "        return {\n",
    "            'n_estimators': n_estimators,\n",
    "            'learning_rate': learning_rate,\n",
    "            'max_depth': max_depth,\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'min_samples_leaf': min_samples_leaf\n",
    "        }\n",
    "        \n",
    "    elif model_name == \"HistGradientBoostingClassifier\":\n",
    "        learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 0.1)\n",
    "        max_depth = trial.suggest_categorical('max_depth', [5, 10, 15, None])\n",
    "        max_iter = trial.suggest_int('max_iter', 100, 300)\n",
    "        l2_regularization = trial.suggest_loguniform('l2_regularization', 0.001, 5.0)\n",
    "        \n",
    "        return {\n",
    "            'learning_rate': learning_rate,\n",
    "            'max_depth': max_depth,\n",
    "            'max_iter': max_iter,\n",
    "            'l2_regularization': l2_regularization\n",
    "        }\n",
    "\n",
    "    elif model_name == \"AdaBoostClassifier\":\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "        learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.5)\n",
    "        algorithm = trial.suggest_categorical('algorithm', ['SAMME', 'SAMME.R'])\n",
    "        \n",
    "        return {\n",
    "            'n_estimators': n_estimators,\n",
    "            'learning_rate': learning_rate,\n",
    "            'algorithm': algorithm\n",
    "        }\n",
    "        \n",
    "    elif model_name == \"KNeighborsClassifier\":\n",
    "        n_neighbors = trial.suggest_int('n_neighbors', 3, 15)\n",
    "        weights = trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
    "        algorithm = trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree'])\n",
    "        p = trial.suggest_int('p', 1, 2)\n",
    "        leaf_size = trial.suggest_int('leaf_size', 20, 50)\n",
    "        \n",
    "        return {\n",
    "            'n_neighbors': n_neighbors,\n",
    "            'weights': weights,\n",
    "            'algorithm': algorithm,\n",
    "            'p': p,\n",
    "            'leaf_size': leaf_size\n",
    "        }\n",
    "        \n",
    "    elif model_name == \"DecisionTreeClassifier\":\n",
    "        max_depth = trial.suggest_categorical('max_depth', [5, 10, 20, 30, None])\n",
    "        min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "        min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 8)\n",
    "        criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "        max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
    "        \n",
    "        return {\n",
    "            'max_depth': max_depth,\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'min_samples_leaf': min_samples_leaf,\n",
    "            'criterion': criterion,\n",
    "            'max_features': max_features\n",
    "        }\n",
    "    \n",
    "    elif model_name == \"LogisticRegressionCV\":\n",
    "        solver = trial.suggest_categorical('solver', ['lbfgs', 'liblinear', 'saga'])\n",
    "        Cs_count = trial.suggest_int('Cs', 5, 30) \n",
    "        \n",
    "        if solver == 'lbfgs' :\n",
    "            penalty = 'l2' \n",
    "        elif solver in ['liblinear', 'saga']:\n",
    "            penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])\n",
    "        else:\n",
    "            penalty = 'l2' \n",
    "            \n",
    "        max_iter = trial.suggest_int('max_iter', 1000, 3000)\n",
    "        class_weight = trial.suggest_categorical('class_weight', ['balanced', None])\n",
    "        cv_inner = trial.suggest_categorical('cv', [3, 5, 7])\n",
    "        \n",
    "        params = {\n",
    "            'solver': solver,\n",
    "            'penalty': penalty,\n",
    "            'Cs': Cs_count,\n",
    "            'max_iter': max_iter,\n",
    "            'class_weight': class_weight,\n",
    "            'cv': cv_inner\n",
    "        }\n",
    "        \n",
    "        return params\n",
    "        \n",
    "    return {} \n",
    "    \n",
    "classifiers_dict = dict(all_estimators(type_filter='classifier'))\n",
    "\n",
    "def objective(trial, model_name, X_train, y_train, CV, classifiers_dict):\n",
    "    try:\n",
    "        params = suggest_params(trial, model_name)\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "    \n",
    "    if model_name not in classifiers_dict:\n",
    "        return 0.0\n",
    "\n",
    "    ClassifierClass = classifiers_dict[model_name]\n",
    "    try:\n",
    "        model = ClassifierClass(**params, random_state=42)\n",
    "    except (ValueError, TypeError):\n",
    "        return 0.0\n",
    "\n",
    "    try:\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=CV, scoring='precision_macro', n_jobs=-1)\n",
    "        mean_precision = scores.mean()\n",
    "        if np.isnan(mean_precision):\n",
    "            return 0.0\n",
    "        return mean_precision\n",
    "        \n",
    "    except Exception as e:\n",
    "        return 0.0\n",
    "    \n",
    "top_10_classifiers_names = [name for name, _ in sorted_results[:10]] \n",
    "tuned_results_optuna = {}\n",
    "N_TRIALS = 50 \n",
    "\n",
    "print(\"Початок підбору гіперпараметрів за допомогою Optuna (50 спроб)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for name in top_10_classifiers_names:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Обробка моделі: {name}\")\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\", \n",
    "        sampler=TPESampler(seed=42),\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        study.optimize(\n",
    "            lambda trial: objective(trial, name, X_train, y_train, CV, classifiers_dict),\n",
    "            n_trials=N_TRIALS, \n",
    "            show_progress_bar=False,\n",
    "            timeout=1800 \n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Помилка в Optuna Study: {e}\")\n",
    "        continue\n",
    "    try:\n",
    "        best_params = study.best_params\n",
    "        mean_score = study.best_value\n",
    "        if mean_score == 0.0:\n",
    "             raise ValueError(\"Усі спроби повернули 0.0. Тюнінг, ймовірно, провалився.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Помилка при отриманні найкращого результату для {name}: {e}\")\n",
    "        print(\"Пропускаємо цю модель.\")\n",
    "        continue \n",
    "        \n",
    "    try:\n",
    "        ClassifierClass = classifiers_dict[name]\n",
    "        best_model_for_cv = ClassifierClass(**best_params, random_state=42)\n",
    "        cv_scores_final = cross_val_score(best_model_for_cv, X_train, y_train, cv=CV, scoring='precision_macro', n_jobs=-1)\n",
    "        cv_std_score = cv_scores_final.std()\n",
    "        best_model = ClassifierClass(**best_params, random_state=42)\n",
    "        best_model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = best_model.predict(X_train)\n",
    "        y_test_pred = best_model.predict(X_test)\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        \n",
    "        roc_auc_test = np.nan\n",
    "        if hasattr(best_model, 'predict_proba'):\n",
    "            proba_test = best_model.predict_proba(X_test)\n",
    "            if is_multiclass and proba_test.ndim == 2 and proba_test.shape[1] == n_classes:\n",
    "                roc_auc_test = roc_auc_score(y_test, proba_test, multi_class='ovr')\n",
    "        \n",
    "        tuned_results_optuna[name] = {\n",
    "            'best_params': best_params,\n",
    "            'cv_mean_score': mean_score,\n",
    "            'cv_std_score': cv_std_score,\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'roc_auc_test': roc_auc_test,\n",
    "            'model': best_model\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        print(f\"Результати Optuna для {name}:\")\n",
    "        print(f\"{'─'*80}\")\n",
    "        print(f\"Найкращі параметри: {best_params}\")\n",
    "        print(f\"CV precision_macro: {mean_score:.4f}\")\n",
    "        print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "        print(f\"Test ROC-AUC: {roc_auc_test:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Помилка фінального навчання моделі {name}: {e}\")\n",
    "            \n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Optuna тюнінг завершено!\")\n",
    "print(\"=\"*80)"
   ],
   "id": "f221ca41242c4ed3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tuned_results_voting = {}\n",
    "\n",
    "def objective_voting(trial, X_train, y_train, CV, top_models):\n",
    "    \n",
    "    weights = {}\n",
    "    \n",
    "    for i, (name, _) in enumerate(top_models):\n",
    "        weights[name] = trial.suggest_float(f'weight_{name}', 0.5, 3.0, log=False)\n",
    "    \n",
    "    estimators_with_weights = [\n",
    "        (name, model) for name, model in top_models\n",
    "    ]\n",
    "    \n",
    "    voting_model_tuned = VotingClassifier(\n",
    "        estimators=estimators_with_weights,\n",
    "        voting='soft',\n",
    "        weights=[weights[name] for name, _ in top_models]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        scores = cross_val_score(voting_model_tuned, X_train, y_train, cv=CV, scoring='precision_macro', n_jobs=-1)\n",
    "        mean_precision = scores.mean()\n",
    "        return mean_precision\n",
    "        \n",
    "    except Exception as e:\n",
    "        return np.nan\n",
    "\n",
    "N_TRIALS = 50 \n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"ПОЧАТОК ТЮНІНГУ: VotingClassifier (soft) з Optuna ({N_TRIALS} спроб)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "voting_study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "\n",
    "try:\n",
    "    voting_study.optimize(\n",
    "        lambda trial: objective_voting(trial, X_train, y_train, CV, top_models),\n",
    "        n_trials=N_TRIALS, \n",
    "        show_progress_bar=False,\n",
    "        timeout=1800 \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Помилка в Optuna Study: {e}\")\n",
    "\n",
    "best_voting_params = voting_study.best_params\n",
    "best_voting_weights = [best_voting_params[key] for key in best_voting_params if key.startswith('weight_')]\n",
    "\n",
    "best_voting_model = VotingClassifier(\n",
    "    estimators=top_models,\n",
    "    voting='soft',\n",
    "    weights=best_voting_weights\n",
    ")\n",
    "\n",
    "best_voting_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_vote_tuned = best_voting_model.predict(X_train) \n",
    "y_pred_vote_tuned = best_voting_model.predict(X_test)\n",
    "precision_vote_tuned = precision_score(y_test, y_pred_vote_tuned, average=\"macro\", zero_division=0)\n",
    "train_accuracy_vote_tuned = accuracy_score(y_train, y_train_pred_vote_tuned) \n",
    "test_accuracy_vote_tuned = accuracy_score(y_test, y_pred_vote_tuned) \n",
    "\n",
    "scores_final_vote = cross_val_score(best_voting_model, X_train, y_train, cv=CV, scoring='precision_macro', n_jobs=-1)\n",
    "cv_std_score_vote = scores_final_vote.std()\n",
    "\n",
    "roc_auc_test_vote = np.nan\n",
    "if hasattr(best_voting_model, 'predict_proba'):\n",
    "    proba_test_vote = best_voting_model.predict_proba(X_test)\n",
    "    if is_multiclass and proba_test_vote.ndim == 2 and proba_test_vote.shape[1] == n_classes:\n",
    "        roc_auc_test_vote = roc_auc_score(y_test, proba_test_vote, multi_class='ovr')\n",
    "    elif not is_multiclass and proba_test_vote.ndim == 2 and proba_test_vote.shape[1] == 2:\n",
    "        roc_auc_test_vote = roc_auc_score(y_test, proba_test_vote[:, 1])\n",
    "\n",
    "tuned_results_voting['VotingClassifier'] = {\n",
    "    'best_params': best_voting_params,\n",
    "    'cv_mean_score': voting_study.best_value,\n",
    "    'cv_std_score': cv_std_score_vote,\n",
    "    'train_accuracy': train_accuracy_vote_tuned,\n",
    "    'test_accuracy': test_accuracy_vote_tuned,\n",
    "    'roc_auc_test': roc_auc_test_vote,\n",
    "    'model': best_voting_model,\n",
    "    'precision (Test, Tuned)': precision_vote_tuned,\n",
    "}\n",
    "\n",
    "\n",
    "print(f\"\\n{'─'*80}\")\n",
    "print(\"РЕЗУЛЬТАТИ ТЮНІНГУ VotingClassifier:\")\n",
    "print(f\"{'─'*80}\")\n",
    "print(f\"Найкращі ваги: {best_voting_params}\")\n",
    "print(f\"CV precision_macro (Best): {voting_study.best_value:.4f}\")\n",
    "print(f\"CV precision_macro (Std): {cv_std_score_vote:.4f}\")\n",
    "print(f\"Train Accuracy: {train_accuracy_vote_tuned:.4f}\") \n",
    "print(f\"Test Accuracy: {test_accuracy_vote_tuned:.4f}\") \n",
    "print(f\"Precision (Test, Tuned) = {precision_vote_tuned:.4f}\")\n",
    "print(f\"ROC-AUC Test: {roc_auc_test_vote:.4f}\") "
   ],
   "id": "2f1dd60bb98c74df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tuned_results_stacking = {}\n",
    "\n",
    "def objective_stacking(trial, X_train, y_train, CV, top_models):\n",
    "    solver = trial.suggest_categorical('solver', ['lbfgs', 'liblinear', 'saga'])\n",
    "    \n",
    "    if solver == 'lbfgs':\n",
    "        penalty = 'l2'\n",
    "    elif solver in ['liblinear', 'saga']:\n",
    "        penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])\n",
    "    else:\n",
    "        penalty = 'l2' \n",
    "        \n",
    "    C = trial.suggest_float('C', 0.001, 100.0, log=True)\n",
    "    max_iter = trial.suggest_int('max_iter', 1000, 3000)\n",
    "    \n",
    "    final_estimator = LogisticRegression(\n",
    "        solver=solver, \n",
    "        penalty=penalty, \n",
    "        C=C, \n",
    "        max_iter=max_iter, \n",
    "        random_state=42,\n",
    "        class_weight='balanced', \n",
    "        multi_class='ovr' \n",
    "    )\n",
    "    \n",
    "    stacked_model_tuned = StackingClassifier(\n",
    "        estimators=top_models,\n",
    "        final_estimator=final_estimator,\n",
    "        cv=3, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        scores = cross_val_score(stacked_model_tuned, X_train, y_train, cv=CV, scoring='precision_macro', n_jobs=-1)\n",
    "        mean_precision = scores.mean()\n",
    "        if np.isnan(mean_precision):\n",
    "            return 0.0\n",
    "        return mean_precision\n",
    "        \n",
    "    except Exception as e:\n",
    "        return 0.0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"ПОЧАТОК ТЮНІНГУ: StackingClassifier Final Estimator ({N_TRIALS} спроб)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "stacking_study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "\n",
    "try:\n",
    "    stacking_study.optimize(\n",
    "        lambda trial: objective_stacking(trial, X_train, y_train, CV, top_models),\n",
    "        n_trials=N_TRIALS, \n",
    "        show_progress_bar=False,\n",
    "        timeout=1800 \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Помилка в Optuna Study: {e}\")\n",
    "\n",
    "best_stacking_params = stacking_study.best_params\n",
    "best_solver = best_stacking_params['solver']\n",
    "\n",
    "if best_solver == 'lbfgs':\n",
    "    best_penalty = 'l2'\n",
    "elif best_solver in ['liblinear', 'saga']:\n",
    "    best_penalty = best_stacking_params['penalty']\n",
    "else:\n",
    "    best_penalty = 'l2'\n",
    "\n",
    "best_final_estimator = LogisticRegression(\n",
    "    solver=best_solver,\n",
    "    penalty=best_penalty,\n",
    "    C=best_stacking_params['C'],\n",
    "    max_iter=best_stacking_params['max_iter'],\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    multi_class='ovr'\n",
    ")\n",
    "\n",
    "best_stacked_model = StackingClassifier(\n",
    "    estimators=top_models,\n",
    "    final_estimator=best_final_estimator\n",
    ")\n",
    "\n",
    "best_stacked_model.fit(X_train, y_train)\n",
    "y_train_pred_stack_tuned = best_stacked_model.predict(X_train)\n",
    "y_pred_stack_tuned = best_stacked_model.predict(X_test)\n",
    "precision_stack_tuned = precision_score(y_test, y_pred_stack_tuned, average=\"macro\", zero_division=0)\n",
    "train_accuracy_stack_tuned = accuracy_score(y_train, y_train_pred_stack_tuned)\n",
    "test_accuracy_stack_tuned = accuracy_score(y_test, y_pred_stack_tuned)\n",
    "scores_final_stack = cross_val_score(best_stacked_model, X_train, y_train, cv=CV, scoring='precision_macro', n_jobs=-1)\n",
    "cv_std_score_stack = scores_final_stack.std()\n",
    "\n",
    "roc_auc_test_stack = np.nan\n",
    "if hasattr(best_stacked_model, 'predict_proba'):\n",
    "    proba_test_stack = best_stacked_model.predict_proba(X_test)\n",
    "    if is_multiclass and proba_test_stack.ndim == 2 and proba_test_stack.shape[1] == n_classes:\n",
    "        roc_auc_test_stack = roc_auc_score(y_test, proba_test_stack, multi_class='ovr')\n",
    "    elif not is_multiclass and proba_test_stack.ndim == 2 and proba_test_stack.shape[1] == 2:\n",
    "        roc_auc_test_stack = roc_auc_score(y_test, proba_test_stack[:, 1])\n",
    "\n",
    "tuned_results_stacking['StackingClassifier'] = {\n",
    "    'best_params': best_stacking_params,\n",
    "    'cv_mean_score': stacking_study.best_value,\n",
    "    'cv_std_score': cv_std_score_stack,\n",
    "    'train_accuracy': train_accuracy_stack_tuned,\n",
    "    'test_accuracy': test_accuracy_stack_tuned,\n",
    "    'roc_auc_test': roc_auc_test_stack,\n",
    "    'model': best_stacked_model,\n",
    "    'precision (Stacking, Tuned)': precision_stack_tuned,\n",
    "}\n",
    "\n",
    "print(f\"\\n{'─'*80}\")\n",
    "print(\"РЕЗУЛЬТАТИ ТЮНІНГУ StackingClassifier:\")\n",
    "print(f\"{'─'*80}\")\n",
    "print(f\"Найкращі параметри Final Estimator: {best_stacking_params}\")\n",
    "print(f\"CV precision_macro (Best): {stacking_study.best_value:.4f}\")\n",
    "print(f\"CV precision_macro (Std): {cv_std_score_stack:.4f}\") \n",
    "print(f\"Train Accuracy: {train_accuracy_stack_tuned:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_stack_tuned:.4f}\")\n",
    "print(f\"Precision (Stacking, Tuned) = {precision_stack_tuned:.4f}\")\n",
    "print(f\"ROC-AUC Test: {roc_auc_test_stack:.4f}\")    "
   ],
   "id": "d74963fecfbc8387"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results_comparison = pd.DataFrame({\n",
    "    'Model': list(tuned_results.keys()),\n",
    "    'CV precision_macro': [tuned_results[m]['cv_mean_score'] for m in tuned_results],\n",
    "    'CV Std': [tuned_results[m]['cv_std_score'] for m in tuned_results],\n",
    "    'Train Accuracy': [tuned_results[m]['train_accuracy'] for m in tuned_results],\n",
    "    'Test Accuracy': [tuned_results[m]['test_accuracy'] for m in tuned_results],\n",
    "    'ROC-AUC Test': [tuned_results[m]['roc_auc_test'] for m in tuned_results],\n",
    "    'Overfit (Train-Test)': [tuned_results[m]['train_accuracy'] - tuned_results[m]['test_accuracy'] \n",
    "                              for m in tuned_results]\n",
    "})\n",
    "\n",
    "results_comparison_optuna = pd.DataFrame({\n",
    "    'Model': list(tuned_results_optuna.keys()),\n",
    "    'CV precision_macro': [tuned_results_optuna[m]['cv_mean_score'] for m in tuned_results_optuna],\n",
    "    'CV Std': [tuned_results_optuna[m]['cv_std_score'] for m in tuned_results_optuna],\n",
    "    'Train Accuracy': [tuned_results_optuna[m]['train_accuracy'] for m in tuned_results_optuna],\n",
    "    'Test Accuracy': [tuned_results_optuna[m]['test_accuracy'] for m in tuned_results_optuna],\n",
    "    'ROC-AUC Test': [tuned_results_optuna[m]['roc_auc_test'] for m in tuned_results_optuna],\n",
    "    'Overfit (Train-Test)': [tuned_results_optuna[m]['train_accuracy'] - tuned_results_optuna[m]['test_accuracy'] \n",
    "                              for m in tuned_results_optuna]\n",
    "})\n",
    "\n",
    "results_comparison_voting = pd.DataFrame({\n",
    "    'Model': 'VotingClassifier',\n",
    "    'CV precision_macro': [tuned_results_voting['VotingClassifier']['cv_mean_score']],\n",
    "    'CV Std': [tuned_results_voting['VotingClassifier']['cv_std_score']],\n",
    "    'Train Accuracy': [tuned_results_voting['VotingClassifier']['train_accuracy']],\n",
    "    'Test Accuracy': [tuned_results_voting['VotingClassifier']['test_accuracy']],\n",
    "    'ROC-AUC Test': [tuned_results_voting['VotingClassifier']['roc_auc_test']],\n",
    "    'Precision': [tuned_results_voting['VotingClassifier']['precision (Test, Tuned)']],\n",
    "    'Overfit (Train-Test)': [tuned_results_voting['VotingClassifier']['train_accuracy'] - tuned_results_voting['VotingClassifier']['test_accuracy']]\n",
    "})\n",
    "\n",
    "results_comparison_stacking = pd.DataFrame({\n",
    "    'Model': 'StackingClassifier',\n",
    "    'CV precision_macro': [tuned_results_stacking['StackingClassifier']['cv_mean_score']],\n",
    "    'CV Std': [tuned_results_stacking['StackingClassifier']['cv_std_score']],\n",
    "    'Train Accuracy': [tuned_results_stacking['StackingClassifier']['train_accuracy']],\n",
    "    'Test Accuracy': [tuned_results_stacking['StackingClassifier']['test_accuracy']],\n",
    "    'ROC-AUC Test': [tuned_results_stacking['StackingClassifier']['roc_auc_test']],\n",
    "    'Precision': [tuned_results_stacking['StackingClassifier']['precision (Stacking, Tuned)']],\n",
    "    'Overfit (Train-Test)': [tuned_results_stacking['StackingClassifier']['train_accuracy'] - tuned_results_stacking['StackingClassifier']['test_accuracy']]\n",
    "})\n",
    "\n",
    "results_comparison = results_comparison.sort_values('Test Accuracy', ascending=False)\n",
    "results_comparison_optuna = results_comparison_optuna.sort_values('Test Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ПІДСУМКОВЕ ПОРІВНЯННЯ МОДЕЛЕЙ\")\n",
    "print(\"=\"*80)\n",
    "print(\"GRIDSEARCH\")\n",
    "print(results_comparison.to_string(index=False))\n",
    "print(\"\\nOPTUNA\")\n",
    "print(results_comparison_optuna.to_string(index=False))\n",
    "print(\"\\nVOTING CLASSIFIER\")\n",
    "print(results_comparison_voting.to_string(index=False))\n",
    "print(\"\\nSTACKING CLASSIFIER\")\n",
    "print(results_comparison_stacking.to_string(index=False))\n",
    "\n",
    "print(\"\\nGRIDSEARCH RESULTS\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "ax1 = axes[0, 0]\n",
    "models = results_comparison['Model']\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, results_comparison['Train Accuracy'], width, label='Train', alpha=0.8)\n",
    "ax1.bar(x + width/2, results_comparison['Test Accuracy'], width, label='Test', alpha=0.8)\n",
    "ax1.set_xlabel('Моделі')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Train vs Test Accuracy')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2 = axes[0, 1]\n",
    "ax2.barh(models, results_comparison['ROC-AUC Test'], alpha=0.8, color='coral')\n",
    "ax2.set_xlabel('ROC-AUC Score')\n",
    "ax2.set_title('ROC-AUC на тестовій вибірці')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "ax3 = axes[1, 0]\n",
    "colors = ['red' if x > 0.05 else 'green' for x in results_comparison['Overfit (Train-Test)']]\n",
    "ax3.barh(models, results_comparison['Overfit (Train-Test)'], alpha=0.8, color=colors)\n",
    "ax3.set_xlabel('Overfit (Train - Test)')\n",
    "ax3.set_title('Оверфітинг моделей (червоний > 0.05)')\n",
    "ax3.axvline(x=0.05, color='black', linestyle='--', linewidth=1)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "ax4 = axes[1, 1]\n",
    "ax4.barh(models, results_comparison['CV precision_macro'], \n",
    "         xerr=results_comparison['CV Std'], \n",
    "         alpha=0.8, color='skyblue', capsize=5)\n",
    "ax4.set_xlabel('CV precision_macro')\n",
    "ax4.set_title('Cross-Validation precision_macro (± std)')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('grid_search_results.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOPTUNA RESULTS\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "ax1 = axes[0, 0]\n",
    "models = results_comparison_optuna['Model']\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, results_comparison_optuna['Train Accuracy'], width, label='Train', alpha=0.8)\n",
    "ax1.bar(x + width/2, results_comparison_optuna['Test Accuracy'], width, label='Test', alpha=0.8)\n",
    "ax1.set_xlabel('Моделі')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Train vs Test Accuracy')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2 = axes[0, 1]\n",
    "ax2.barh(models, results_comparison_optuna['ROC-AUC Test'], alpha=0.8, color='coral')\n",
    "ax2.set_xlabel('ROC-AUC Score')\n",
    "ax2.set_title('ROC-AUC на тестовій вибірці')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "ax3 = axes[1, 0]\n",
    "colors = ['red' if x > 0.05 else 'green' for x in results_comparison_optuna['Overfit (Train-Test)']]\n",
    "ax3.barh(models, results_comparison_optuna['Overfit (Train-Test)'], alpha=0.8, color=colors)\n",
    "ax3.set_xlabel('Overfit (Train - Test)')\n",
    "ax3.set_title('Оверфітинг моделей (червоний > 0.05)')\n",
    "ax3.axvline(x=0.05, color='black', linestyle='--', linewidth=1)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "ax4 = axes[1, 1]\n",
    "ax4.barh(models, results_comparison_optuna['CV precision_macro'], \n",
    "         xerr=results_comparison_optuna['CV Std'], \n",
    "         alpha=0.8, color='skyblue', capsize=5)\n",
    "ax4.set_xlabel('CV precision_macro')\n",
    "ax4.set_title('Cross-Validation precision_macro (± std)')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('optuna_results.png', dpi=300)\n",
    "plt.show()"
   ],
   "id": "e589881490410d94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "final_combined_results = pd.concat([\n",
    "        results_comparison_optuna,\n",
    "        results_comparison_voting,\n",
    "        results_comparison_stacking\n",
    "    ], ignore_index=True)\n",
    "final_combined_results = final_combined_results.sort_values('Test Accuracy', ascending=False).round(4).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ОБ'ЄДНАНІ РЕЗУЛЬТАТИ: Індивідуальні Optuna та Ансамблеві моделі\")\n",
    "print(\"=\"*80)\n",
    "print(final_combined_results.to_string(index=False))"
   ],
   "id": "ac7695be0903b6ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "print(\"Побудова ROC-кривих для Top-10 моделей...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name in tuned_results.keys():\n",
    "    try:\n",
    "        best_model = tuned_results[name]['model']\n",
    "        tprs, aucs = [], []\n",
    "\n",
    "        for fold, (train_index, val_index) in enumerate(CV.split(X_train, y_train), 1):\n",
    "            X_tr, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "            y_tr, y_val = y_train[train_index], y_train[val_index]\n",
    "            \n",
    "            from sklearn.base import clone\n",
    "            model_fold = clone(best_model)\n",
    "            model_fold.fit(X_tr, y_tr)\n",
    "\n",
    "            if hasattr(model_fold, 'predict_proba'):\n",
    "                y_scores = model_fold.predict_proba(X_val)\n",
    "            elif hasattr(model_fold, 'decision_function'):\n",
    "                y_scores = model_fold.decision_function(X_val)\n",
    "                if y_scores.ndim == 1:\n",
    "                    y_scores = np.vstack([1 - y_scores, y_scores]).T\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if is_multiclass:\n",
    "                y_val_bin = label_binarize(y_val, classes=np.arange(n_classes))\n",
    "                if y_scores.shape[1] == n_classes:\n",
    "                    fpr, tpr, _ = roc_curve(y_val_bin.ravel(), y_scores.ravel())\n",
    "                    mean_auc = auc(fpr, tpr)\n",
    "                    aucs.append(mean_auc)\n",
    "                    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "            else:\n",
    "                if y_scores.ndim == 2:\n",
    "                    y_scores = y_scores[:, 1]\n",
    "                fpr, tpr, _ = roc_curve(y_val, y_scores)\n",
    "                aucs.append(auc(fpr, tpr))\n",
    "                tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "\n",
    "        if not tprs:\n",
    "            print(f\"  {name}: Неможливо побудувати ROC-криву\")\n",
    "            continue\n",
    "\n",
    "        mean_tpr = np.mean(tprs, axis=0)\n",
    "        mean_tpr[-1] = 1.0\n",
    "        mean_auc = np.mean(aucs)\n",
    "        std_auc = np.std(aucs)\n",
    "\n",
    "        plt.plot(mean_fpr, mean_tpr, \n",
    "                label=f\"{name} (AUC = {mean_auc:.3f} ± {std_auc:.3f})\",\n",
    "                linewidth=2)\n",
    "        print(f\"  {name}: AUC = {mean_auc:.3f} ± {std_auc:.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  {name}: Помилка - {e}\")\n",
    "        continue\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', \n",
    "         label='Random (AUC = 0.5)', linewidth=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC-криві (Top-10 класифікаторів з оптимальними параметрами)', fontsize=14)\n",
    "plt.legend(loc='lower right', fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curves.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ROC-криві побудовано!\")"
   ],
   "id": "30246bbee1670cb4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "best_model_name = results_comparison_optuna.iloc[0]['Model']\n",
    "best_model_obj = tuned_results_optuna[best_model_name]['model']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"ДЕТАЛЬНИЙ АНАЛІЗ НАЙКРАЩОЇ МОДЕЛІ: {best_model_name}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "y_train_pred = best_model_obj.predict(X_train)\n",
    "y_test_pred = best_model_obj.predict(X_test)\n",
    "\n",
    "print(\"\\nClassification Report на тренувальній вибірці:\")\n",
    "print(\"-\" * 40)\n",
    "print(classification_report(y_train, y_train_pred, target_names=le_y.classes_))\n",
    "\n",
    "print(\"\\nClassification Report на тестовій вибірці:\")\n",
    "print(\"-\"*80)\n",
    "print(classification_report(y_test, y_test_pred, target_names=le_y.classes_))\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=le_y.classes_)\n",
    "disp_train.plot(ax=ax1, cmap='Greens', values_format='d')\n",
    "ax1.set_title(f'Confusion Matrix (Train) - {best_model_name}')\n",
    "\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=le_y.classes_)\n",
    "disp_test.plot(ax=ax2, cmap='Blues', values_format='d')\n",
    "ax2.set_title(f'Confusion Matrix (Test) - {best_model_name}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"ВАЖЛИВІСТЬ ОЗНАК ДЛЯ {best_model_name}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if best_model_name in feature_importance_dict:\n",
    "    importances = feature_importance_dict[best_model_name]\n",
    "    feature_imp_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(feature_imp_df.to_string(index=False))\n",
    "    \n",
    "    top_n = min(20, len(feature_imp_df))\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(top_n), feature_imp_df['Importance'].head(top_n), alpha=0.8)\n",
    "    plt.yticks(range(top_n), feature_imp_df['Feature'].head(top_n))\n",
    "    plt.xlabel('Важливість')\n",
    "    plt.title(f'Top-{top_n} найважливіших ознак - {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance.png', dpi=300)\n",
    "    plt.show()"
   ],
   "id": "6e5224b27f518c25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def perform_error_analysis(X, y_true, y_pred, model, le, dataset_name=\"Test\", filename=\"plot.png\"):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"АНАЛІЗ ПОМИЛКОВИХ ПЕРЕДБАЧЕНЬ ({dataset_name.upper()})\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    wrong_indices = np.where(y_true != y_pred)[0]\n",
    "    num_errors = len(wrong_indices)\n",
    "    \n",
    "    if num_errors > 0:\n",
    "        X_errors = X.iloc[wrong_indices].copy()\n",
    "        errors_df = pd.DataFrame({\n",
    "            'Index': X.iloc[wrong_indices].index,\n",
    "            'True_Class': le.inverse_transform(y_true[wrong_indices]),\n",
    "            'Predicted_Class': le.inverse_transform(y_pred[wrong_indices])\n",
    "        })\n",
    "        \n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            proba = model.predict_proba(X_errors)\n",
    "            errors_df['Confidence'] = proba.max(axis=1)\n",
    "            \n",
    "        full_errors_df = pd.concat([errors_df, X_errors], axis=1)\n",
    "        print(f\"\\nКількість помилок: {num_errors} з {len(y_true)} ({num_errors/len(y_true)*100:.2f}%)\")\n",
    "        print(f\"\\nПЕРШІ 10 РЯДКІВ ІЗ ПОМИЛКОВИМИ ЗНАЧЕННЯМИ ({dataset_name}):\")\n",
    "        print(\"-\" * 80)\n",
    "        display(full_errors_df.head(10))\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    " \n",
    "        ax1 = axes[0]\n",
    "        true_class_errors = full_errors_df['True_Class'].value_counts()\n",
    "        ax1.bar(true_class_errors.index, true_class_errors.values, alpha=0.8, color='coral')\n",
    "        ax1.set_title(f'Які класи модель пропускає ({dataset_name})')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "        ax2 = axes[1]\n",
    "        pred_class_errors = full_errors_df['Predicted_Class'].value_counts()\n",
    "        ax2.bar(pred_class_errors.index, pred_class_errors.values, alpha=0.8, color='skyblue')\n",
    "        ax2.set_title(f'У які класи модель помилково відносить ({dataset_name})')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Графік збережено як: {filename}\")\n",
    "        \n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Помилок у наборі {dataset_name} не виявлено!\")\n",
    "\n",
    "perform_error_analysis(X_train, y_train, y_train_pred, best_model_obj, le_y, dataset_name=\"Train\", filename=\"errors_train_analysis.png\")\n",
    "perform_error_analysis(X_test, y_test, y_test_pred, best_model_obj, le_y, dataset_name=\"Test\", filename=\"errors_test_analysis.png\")"
   ],
   "id": "ae650d7ba1b8e4e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def deep_error_reason_analysis(X, y_true, y_pred, model, dataset_name=\"Test\"):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"ГЛИБОКИЙ АНАЛІЗ ПРИЧИН ПОМИЛОК ({dataset_name.upper()})\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    wrong_idx = np.where(y_true != y_pred)[0]\n",
    "    correct_idx = np.where(y_true == y_pred)[0]\n",
    "    \n",
    "    if len(wrong_idx) == 0:\n",
    "        print(f\"Немає помилок для аналізу в наборі {dataset_name}.\")\n",
    "        return\n",
    "\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        all_probs = model.predict_proba(X)\n",
    "        confidences = all_probs.max(axis=1)\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(confidences[correct_idx], bins=30, alpha=0.5, label='Правильні передбачення', color='green')\n",
    "        plt.hist(confidences[wrong_idx], bins=30, alpha=0.5, label='Помилкові передбачення', color='red')\n",
    "        plt.axvline(confidences[wrong_idx].mean(), color='darkred', linestyle='--', label='Сер. впевненість помилок')\n",
    "        plt.title(f'Розподіл впевненості моделі ({dataset_name})')\n",
    "        plt.xlabel('Впевненість (max probability)')\n",
    "        plt.ylabel('Кількість прикладів')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.2)\n",
    "        plt.savefig(f'confidence_distribution_{dataset_name}.png', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "    mean_errors = X.iloc[wrong_idx].mean()\n",
    "    mean_total = X.mean()\n",
    "    \n",
    "    feature_diff = ((mean_errors - mean_total) / (mean_total + 1e-9)) * 100\n",
    "    \n",
    "    diff_df = pd.DataFrame({\n",
    "        'Mean_in_Errors': mean_errors,\n",
    "        'Mean_Overall': mean_total,\n",
    "        'Difference_%': feature_diff\n",
    "    }).sort_values(by='Difference_%', key=abs, ascending=False)\n",
    "\n",
    "    print(f\"\\nТоп ознак, що найбільше відрізняються в помилкових прогнозах ({dataset_name}):\")\n",
    "    print(\"Це вказує на те, які фактори найчастіше корелюють із помилкою.\")\n",
    "    print(\"-\" * 80)\n",
    "    display(diff_df.head(15))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_diffs = diff_df.head(10)\n",
    "    colors = ['red' if x > 0 else 'blue' for x in top_diffs['Difference_%']]\n",
    "    plt.barh(top_diffs.index, top_diffs['Difference_%'], color=colors, alpha=0.7)\n",
    "    plt.xlabel('Відхилення від середнього (%)')\n",
    "    plt.title(f'Аномалії в ознаках помилкових прогнозів ({dataset_name})')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.savefig(f'feature_anomaly_{dataset_name}.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "deep_error_reason_analysis(X_train, y_train, y_train_pred, best_model_obj, dataset_name=\"Train\")\n",
    "deep_error_reason_analysis(X_test, y_test, y_test_pred, best_model_obj, dataset_name=\"Test\")"
   ],
   "id": "58ee008033572742"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import shap\n",
    "\n",
    "def perform_shap_analysis(model, X_train, X_test, feature_names):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SHAP ANALYSIS (INTERPRETABILITY)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    try:\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "    except:\n",
    "        explainer = shap.KernelExplainer(model.predict_proba, shap.sample(X_train, 100))\n",
    "        shap_values = explainer.shap_values(X_test.iloc[:100])\n",
    "\n",
    "    print(\"\\n[INFO] Побудова Summary Plot (Загальний вплив ознак)...\")\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, X_test, feature_names=feature_names, show=False)\n",
    "    plt.title(\"SHAP Summary Plot - Загальний внесок ознак\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('shap_summary_plot.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    sample_idx = 0 \n",
    "    print(f\"\\n[INFO] Побудова Waterfall Plot для конкретного прикладу (Index: {X_test.index[sample_idx]})...\")\n",
    "    class_idx = 0 \n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    if isinstance(shap_values, list):\n",
    "        expected_value = explainer.expected_value[class_idx]\n",
    "        current_shap_values = shap_values[class_idx][sample_idx]\n",
    "    else:\n",
    "        expected_value = explainer.expected_value\n",
    "        current_shap_values = shap_values[sample_idx]\n",
    "\n",
    "    shap.plots._waterfall.waterfall_legacy(expected_value, current_shap_values, feature_names=feature_names, show=False)\n",
    "    plt.title(f\"Логіка прогнозу для класу {class_idx} (Рядок {sample_idx})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('shap_waterfall_plot.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "perform_shap_analysis(best_model_obj, X_train, X_test, feature_names)"
   ],
   "id": "db15dfe2e6f92844"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
